#!/bin/bash -eu


################################################################################
# 
#   'bred': 'bashreduce', mapreduce in bash (by erik@fawx.com), enhanced
#
#                                                      dakusui@gmail.com
#
#   Apr/8/2015 Changes from previous version:
#   * Introduced "bredfs" (bred's file system)
#   * Introduced "conf" file. 
#   * Introduced a mechanism to determine unused job number (and port numbers
#     to be used for the job)
#   * Removed -p (base port option) - use conf file entry 'baseport'
#   * Removed -t (tmp dir option) - use conf file entry 'sorttmpdir' for sort. see also workdir, jmdir, and fsdir.
#   * Removed -S (buffer memory for sort) - use conf file entry 'sortmem' and/or 'defaultsortmem'
#   * Removed -j (job id option) - no alternative
#   * CAUTION: "conf" file is 'sourced' and you can do anything. You must
#              understand what you are doing when you change it.
# 
#   Feb/26/2015 Changes from 'bashreduce':
#   * Abolished 'brm': I gave up making it consistent with 'sort -m'.
#   * Created 'map' and 'reduce' behaviour modes, which allow you to perform
#     tasks more normal mapreduce way. The behavior erikfrey created is now 
#     called 'compat' and it is a default.
#   * Made it possible to specify job_id (-j option) by an integer. Based on this
#     number bred allocates ports to be used and now you can use bred multiple 
#     times in a command line connecting them pipes.
#   * Made it possible to specify base port.
#   * Added 'sort_opt' option to be able to make sort commands use it. This is
#     useful e.g., when you want to sort the output by a numeric field. Use "-s '-n'"
#   * Reorganized internal structure.
# 
################################################################################

usage() {
  local prog="`basename $1`"
  echo "Usage: $prog [-m host1 [host2...]] [-c column] [-r userfunc] [-i input] [-o output] [-e error]"
  echo "       $prog -h for help."
  exit 2
}

showhelp() {
  echo "Usage: `basename $1`: [-m host1 [host2...]] [-c column] [-r userfunc] [-i input] [-o output] [-e error]"
  echo   "bred. bashreduce enhanced.  Map an input file to many hosts, sort/reduce, merge"
  echo   "  -m: hosts to use, can repeat hosts for multiple cores"
  echo   "      default hosts from ~/.br.hosts or /etc/br.hosts"
  echo   "  -M: mode ('map', 'reduce', 'compat'), default = 'compat'"
  echo   "  -I: interpreter. ignored if 'compat' mode is used, default = 'bash'"
  echo   "  -c: column to partition, default = $mapcolumn (1-based)"
  echo   "  -i: input file, default = stdin"
  echo   "  -o: output file, default = stdout"
  echo   "  -e: error file, default = stderr"
  echo   "  -r: userfunc function, default = identity"
  echo   "  -f: read userfunc from a file (yes/no), default = no"
  echo   "  -s: key to 's'ort(s) on reduce (and following merge). , default is equal to -c (e.g. $mapcolumn,$mapcolumn) "
  echo   "  -T: option given to sor'T'(s) on reduce (and following merge), default is nothing"
  echo   "  -O: perform sort on output, ignored in 'compat' mode., default = yes"
  echo   "  -h: this help message"
  exit 2
} 

####
# Writes a debug message
debug() {
  if [ "${BRED_DEBUG:-off}" == "on" ]; then
    echo "${prog}: DEBUG: ${1}" >&2
  fi
}

####
# Checks if debug mode is enabled
is_debug() {
  if [ "${BRED_DEBUG:-off}" == "on" ]; then
    return 0
  fi
  return 1
}

####
# Generates job index
# Return:
# - -1: Failed to generate job index.
# - 0, 1, ...: Valid job id
bred_gen_job_idx() {
  # job_idx
  local _nextid=-1
  local _jid_dir="${jmdir}"
  local _job_idx=
  for i in $(seq 1 3); do
    # Sleep randomized milliseconds to increase probability not to hit
    # race condition.
    sleep $(printf '0.0%s' $RANDOM)
    _job_idx="$(_nextid=$(echo $(($(ls ${_jid_dir}| sort -n -r  | head --lines=1) + 1))) \
           && mkdir ${_jid_dir}/${_nextid} && echo ${_nextid})" 2>&1
    debug "try: $i: $_job_idx"
    if [[ -n "${_job_idx}" ]]; then
      echo ${_job_idx}
      return 0
    fi
  done
  echo -1
  return 1
}

# int
#   
# out
#   jobid    "${OPTARG:-$(uuid)}"
#   jobpath  "${tmp_dir:-$HOME/.bred}/jm/${job_idx}/job"
#   nodepath "${tmp_dir:-$HOME/.bred}/jm/${job_idx}/nodes/${host_idx}"
#   hosts_fromopt
# local
bred_parseopt() {
  LC_ALL=${LC_ALL:-""}
  job_idx=-1
  mode='compat'
  interpreter='bash'
  mapcolumn=1
  userfunc=
  input=
  output=
  error=
  sort_key=
  sort_on_out="yes"
  sort_opt=
  hosts_fromopt=
  local _fromfile="no"
  
  while getopts "m:M:I:c:f:r:i:o:e:s:T:O:h" name; do
    case "$name" in
      m)  hosts_fromopt=${OPTARG};;
      M)  mode=$OPTARG;;
      I)  interpreter=$OPTARG;;
      c)  mapcolumn=$OPTARG;;
      r)  userfunc=$OPTARG;;
      f)  _fromfile=$OPTARG;;
      i)  input=$OPTARG;;
      o)  output=$OPTARG;;
      e)  error=$OPTARG;;
      s)  sort_key=$OPTARG;;
      O)  sort_on_out=$OPTARG;;
      T)  sort_opt=$OPTARG;;
      h)  showhelp $0;;
      [?])  usage $0;;
    esac
  done
  if [[ "${_fromfile}" == "yes" ]]; then
    debug "bred_parseopt: before--->${userfunc}:"
    userfunc=`cat "${userfunc}"`
    debug "bred_parseopt: after--->${userfunc}:"
  fi
  sort_key=${sort_key:-$mapcolumn,$mapcolumn}
  case "$mode" in
    compat|map|reduce)
      ;;
    *)
      echo "`basename $0`: unknown mode is specified. ($mode)"
      usage $0
      ;;
  esac
  job_id=${jobid:-"`uuidgen`"}
}

###
# Prints sort buffer memory for specified host.
# The host is specified by 'index' not the name.
# * Parameters
# - _host_idx
bred_get_sortmem() {
  local _hostname="${hosts[$1]}"
  if [[ -n "${sortmem[_hostname]+isset}" ]]; then
    echo "${sortmem[_hostname]}"
    return 0
  fi
  echo "${defaultsortmem}"
  return 0
}

# * in
#   (params)     mode, host_idx, nodepath
#   (enviromnet) LC_ALL
#   (from opt)   mapcolumn, interpreter, userfunc, sortmem, sorttmpdir
# * out
#   task,usercode
# * local
#   sorter, envvars
bred_compose_task() {
  debug "bred_compose_task: START"
  # mode: 'map', 'reduce', 'compat'
  local mode=$1
  local host_idx=$2
  local usercode_errlog="${3}/${host_idx}/err-usercode.log"
  local sorter_errolog="${3}/${host_idx}/err-sorter.log"
  
  sorter="2>$sorter_errolog"
  if [ "$sort_on_out" == "yes" ] ; then
    local _sortmem=$(bred_get_sortmem "$host_idx")
    sorter="| LC_ALL='$LC_ALL' sort -S${_sortmem} -T${sorttmpdir} -k${sort_key} ${sort_opt}  2>${sorter_errolog}"
	echo $sorter >&2
  fi
  
  local envvars=
  local _namenode=${namenode}
  local _hosts="${hosts}"
  local _usercode_path="${nodepath}/${host_idx}/shared/usercode"
  local _bredcore_path="${nodepath}/${host_idx}/shared/bred-core"
  local _brhosts_path="${nodepath}/${host_idx}/shared/br.hosts"
  debug "bred_compose_task: INFO: (1)"
  usercode=
  envvars="${envvars} BRED_CORE='${_bredcore_path}'"
  envvars="${envvars} BRED_PARTID=\"$host_idx\" BRED_KEYINDEX=\"$mapcolumn\" BRED_INTERPRETER=\"$interpreter\""
  envvars="${envvars} BRED_USERCODE_PATH='${_usercode_path}'"
  envvars="${envvars} BRED_HOSTS_PATH='${_brhosts_path}'"
  debug "bred_compose_task: INFO: (2)"
  local _i
  for _i in $(set -o posix ; set | grep -E "^BREDVAR_.*$"); do
    _i="${_i#BREDVAR_}"
    debug "bred_compose_task: INFO: ${_i}"
    envvars="${envvars} ${_i}"
  done
  debug "bred_compose_task: INFO: (3)"
  
  case "$mode" in
    map)
      debug "bred_compose_task: map"
      if [[ -z "$userfunc" ]]; then
                echo "`basename $0`: $mode is specified. must specify userfunc with -r"
                usage $1
      fi
      usercode=${userfunc}
      task="| ( $envvars ${interpreter} '${_usercode_path}' $sorter ) "
      ;;
    reduce)
      debug "bred_compose_task: reduce"
      if [[ -z "$userfunc" ]]; then
        echo "`basename $0`: ${mode} is specified. must specify userfunc with -r"
        usage $1
      fi
      case ${interpreter} in
        "awk-native")
	  debug "bred_compose_task: reduce: awk-native"
          usercode='BEGIN {
                        bred_firsttime=1;
                        bred_key=FS;
                        bred_keyindex=ENVIRON["BRED_KEYINDEX"];
                    }
                    {
                        if (bred_key!=$bred_keyindex) {
                          if (bred_firsttime!=1) {
                              bredEndReduce();
                          }
                          bredBeginReduce(bred_keyindex);
                        }
                        bredReduce(bred_keyindex);
                        bred_firsttime=0;
                        bred_key=$bred_keyindex;
                    }
                    END {
                        if (bred_firsttime==0) {
                            bredEndReduce();
                        }
                    }
                    __USERCODE__'
          #
          #  __USERCODE__ is replaced with a string provided by user using '-r' option.
          #  Below is an example to perform 'word count'.
          #
          #  function bredBeginReduce(key_idx) {
          #    i=0;
          #    key=$key_idx;
          #    print "BEGIN", key, "(", key_idx, ")"
          #  }
          #  function bredReduce(key_idx) {
          #     print "REDUCE", $key_idx
          #     i++;
          #  }
          #  function bredEndReduce() {
          #    print "END", key, i;
          #  }
          usercode=${usercode//__USERCODE__/${userfunc}}
          task="| ( $envvars $AWK -f '${_usercode_path}' 2> $usercode_errlog $sorter ) "
          ;;
        *)
	  debug "bred_compose_task: reduce: *"
          local _usercode_executor='BEGIN {
                        cmd=sprintf("%s %c%s%c", ENVIRON["BRED_INTERPRETER"], 39, ENVIRON["BRED_USERCODE_PATH"], 39);
                        firsttime=1;
                        key="";
                        keyindex=ENVIRON["BRED_KEYINDEX"];
                    }
                    {
                        if (key!=$keyindex) {
                          if (firsttime!=1) {
                              close(cmd);
                          }
                        }
                        print | cmd;
                        firsttime=0;
                        key=$keyindex;
                    }
                    END {
                        if (firsttime==0) {
                            close(cmd);
                        }
                    }'
	  usercode=${userfunc}
          task="| ( $envvars $AWK '${_usercode_executor}' 2> $usercode_errlog $sorter ) "
          ;;
      esac
      ;;
    compat)
      local _usercode_executor
      if [ -n "$userfunc" ]; then
        # if we have a userfunc, add the pipe explicitly
        _usercode_executor="| ( $envvars $interpreter '${_usercode_path}' 2> $usercode_errlog ) "
      else
        _usercode_executor=""
      fi
      usercode="${userfunc}"
      task="$sorter ${_usercode_executor}"
      ;;
    *)
      echo "Unknown mode '$1' was given."
      exit 1
      ;;
  esac
  debug "bred_compose_task: END"
}

bred_dump_conf() {
  echo "baseport=${baseport}"
  echo "hosts=(${hosts[@]})"
  echo "namenode=${namenode}"
  echo "workdir=${workdir}"
  echo "fsdir=${fsdir}"
  echo "jmdir=${jmdir}"
  echo "sorttmpdir=${sorttmpdir}"
  local _i
  for _i in "${!sortmem[@]}"; do
    echo "sortmem[${_i}]=${sortmem[${_i}]}"
  done
  echo "defaultsortmem=$defaultsortmem"
}

bred_setup_shared_resources() {
  local _host="$1"
  local _shared_dir="$2"
  local _usercode="$3"
  mkdir -p "${_shared_dir}"
  echo "${_usercode}" > "${_shared_dir}/usercode"
  cp "$(dirname $0)/bred-core" "${_shared_dir}/"
  bred_dump_conf >> "${_shared_dir}/bred.conf"
  debug "bred_setup_shared_resources: INFO: (begin for)"
  truncate -s 0 "${_shared_dir}/br.hosts"
  local _each
  for _each in ${hosts}; do
    debug "bred_setup_shared_resources: INFO: each=${_each}"
    echo "${_each}"
  done >> "${_shared_dir}/br.hosts"
  debug "bred_setup_shared_resources: INFO: (end for)"
  rsync -r "${_shared_dir}"  "${_host}":"$(dirname ${_shared_dir})/"
}

####
# Executes a map/reduce/compat task
# 
# variables:
# * read
#   (from opt) hosts, mode, mapcolumn, userfunc, solrt_column
#   (globals)  sorter, jmdir
# * write
#   (none)
# * local
#   host, pid, envvars, port_in, port_out, host_idx, out_files, in_filed, input, num_hosts
bred_executejob() {
  AWK="/usr/bin/awk"
  # okay let's get started!  first we need a name for our job
  local jobpath="$jmdir/${job_idx}/job"
  local nodepath="$jmdir/${job_idx}/nodes"
  local num_hosts="${#hosts[@]}"
  local port_in=$(($baseport + $num_hosts * $job_idx * 2))
  local port_out=$(($port_in + 1))
  local host_idx=0
  local out_files=
  local in_files=

  debug "bred_executejob: num_hosts=${num_hosts}"
  # now, for each host, set up in and out fifos (and a netcat for each), and ssh to each host to set up workers listening on netcat
  mkdir -p "${jobpath}/"{in,out}
  for host in "${hosts[@]}"; do
    debug "bred_executejob: INFO: begin for"
    ####
    # set up communication path
    # 1) master side
    #    our named pipes
    mkfifo "${jobpath}/"{in,out}"/${host_idx}"
    if [[ "$?" != 0 ]]; then
      echo "FAIL!!!" 2>&1
      exit 1
    fi
    # 2) slave side
    ssh -A -n "$host" "mkdir -p ${nodepath}/${host_idx}"
    #    let's start the remote 'nc' listener and get the pid of it
    local pid=$(ssh -n "$host" "nc -l -p $port_out >${nodepath}/${host_idx}/in 2>/dev/null </dev/null & jobs -l" | awk {'print $2'})
    
    debug "bred_executejob: INFO: for (1.0): ${jobpath}, ${nodepath}"
    ####
    # start task process on remote side and connect to it to the listener
    # 1) compose 'task' component based on execution mode (map, reduce, or compat)
    bred_compose_task "$mode" "$host_idx" "${nodepath}"
    debug "bred_executejob: INFO: for (1.1)"
    bred_setup_shared_resources "${host}" "${nodepath}/${host_idx}/shared" "${usercode}"
    # 2) start the task.
    ssh -A "$host" -n "\
            tail -s0.01 -f --pid=$pid ${nodepath}/${host_idx}/in 2>/dev/null </dev/null \
                    $task \
                    | nc -q0 -l -p $port_in >&/dev/null &"
    
    ####
    # Connect the task to the local forwarder of master side's
    # our local forwarders
    debug "bred_executejob: INFO: for (2)"
    nc "${host}" "${port_in}" >"${jobpath}/in/${host_idx}" &
    debug "bred_executejob: INFO: inbound connected process: $(lsof -t -i TCP:${port_in})"
    nc -q0 "${host}" "${port_out}" <"${jobpath}/out/${host_idx}" &
    debug "bred_executejob: INFO: outbound connected process: $(lsof -t -i TCP:${port_out})"
    # our vars
    debug "bred_executejob: INFO: for (3)"
    out_files="$out_files $jobpath/out/$host_idx"
    in_files="$in_files $jobpath/in/$host_idx"
    port_in=$(($port_in + 2))
    port_out=$(($port_in + 1))
    host_idx=$(($host_idx + 1))
    debug "bred: bred_executejob: INFO: end for: ${host_idx}"
  done

  ####    
  # okay, time to map
  if which brp >/dev/null; then
    debug "bred: bred_executejob: INFO: mapcolumn='${mapcolumn}' out_files='${out_files}'"
    eval "${input:+pv $input |} brp - $((${mapcolumn} - 1)) ${out_files}"
  else
    ####
    # use awk if we don't have 'brp'
    # we're taking advantage of a special property that awk leaves its file handles open until its done
    # i think this is universal
    # we're also sending a zero length string to all the handles at the end, in case some pipe got no love
    eval "${input:+pv $input |} awk '{
            srand(\$$mapcolumn);
            print \$0 >>\"$jobpath/out/\"int(rand() * $host_idx);
        }
        END {
            for (i = 0; i != $host_idx; ++i)
            printf \"\" >>\"$jobpath/out/\"i;
        }'"
  fi
  
  ####    
  # save it somewhere
  # use sort -m. 
  # (dakusui: abolished brm to simplify the software since i couldn't convince myself there is a big performance gain)
  # sort -m creates tmp files if too many input files are specified
  # brm doesn't do this
  ##
  # WARNING:  For some reason, unless in_files and remote temporary files are accessed
  #           before the former are read,
  #           sort/cat sometimes print nothing. This happens intermittently on
  #           Raspberry Pi 2. We do 'ls' here.
  local _in_files_state=$(ls -al ${in_files})
  debug "INFO: in_files: ${_in_files_state}"
  local _each_host
  local _host_idx=0
  for _each_host in "${hosts[@]}"; do
    local _node_state=$(ssh -A "${_each_host}" "ls -al '${nodepath}/${_host_idx}/in'")
    _host_idx=$((${_host_idx} + 1))
    debug "bred_executejob: bred_executejob: node state (${_host_idx}): ${_node_state}"
  done
  # END OF WARNING
  ##

  debug "INFO: merge is started"
  if [ ${sort_on_out} == "yes" ] ; then
    eval "sort -k${sort_key} -m ${in_files} ${sort_opt} ${output:+| pv >$output}"
  else
    eval "cat ${in_files} ${output:+| pv >$output}"
  fi
  debug "INFO: merge is finished"
  
  ####
  # save errors somewhere
  debug "INFO: saving errors"
  local j=0;
  for host in "${hosts[@]}"; do
    local cmd='for i in $(ls '${nodepath}'/'${j}'/err*); do echo '${j}' $(echo $i); nl -w 1 -s " " $i; echo '${j}' ----; done'
    if [ -n "$error" ]; then
      ssh -A "$host" "$cmd" >> "${error}"
    else
      # By default, redirect stdouts of ssh commands to stderr because the files are originally stderr.
      ssh -A "$host" "$cmd" 1>&2
    fi
    j=$(($j + 1))
  done
  debug "INFO: errors are saved"
  
  ####    
  # finally, clean up after ourselves
  debug "INFO: cleaning up"
  if  ! is_debug ; then
    rm -rf "${jobpath:?something goes wrong!}"
    for host in ${hosts}; do
      ssh -A "$host" "rm -fr ${nodepath:?'node path isn't set''}"
    done
    rm -fr "${jmdir:?something goes wrong!}/${job_idx:?something goes wrong!}" >&2
  fi
  debug "INFO: clepan up is finished"
  
  ####    
  # TODO: is there a safe way to kill subprocesses upon fail?
  # this seems to work: /bin/kill -- -$$
}

main() {
  ####
  # Configurable parameters and their default values. Those are defined
  # in bred-core
  # baseport=8192
  # hosts=(localhost localhost localhost localhost)
  # workdir=/tmp/bred
  # fsdir=${workdir}/fs
  # jmdir=${workdir}/jm
  # sorttmpdir=${workdir}/tmp
  # sortmem[localhost]=32M
  # defaultsortmem=256K
  hosts_fromopt=
  namenode=
  workdir=/tmp/bred
  fsdir=${workdir}/fs
  jmdir=${workdir}/jm
  sorttmpdir=${workdir}/tmp
  declare -A sortmem
  sortmem[localhost]=32M
  defaultsortmem=256K
  prog="`basename $0`"
  bred_parseopt "$@"
  debug "main: parseopt finished"
  source "$(dirname $(readlink -e $0))/bred-core"
  ##
  # determine job idx.
  mkdir -p "${jmdir}"
  job_idx=$(bred_gen_job_idx)
  if [[ "${job_idx}" == -1 ]]; then
    quit "Failed to get job id. : ${jmdir}"
  fi
  debug "bred_parseopt: retrieved job id: ${job_idx:?Failed to get job id. }: ${jmdir}"
  echo "$$" > "${jmdir}/${job_idx}/pid"
  ##
  # check if 'hosts' is set.
  debug "main: hosts_fromopt=${hosts_fromopt}"
  if [[ -n "${hosts_fromopt[*]}" ]]; then
    debug "main: hosts is provided from option."
    hosts=()
    IFS=" " read -ra hosts <<< ${hosts_fromopt}
    namenode="${hosts_fromopt[0]}"
    debug "main: hosts is overridden. '${hosts[*]}'"
  fi
  hosts=${hosts:?"`basename $0`: hosts are not given."}
  debug "main: hosts='${hosts[*]}'"
  
  bred_executejob;

  debug "main: job execution is finished."
}

main "$@"
